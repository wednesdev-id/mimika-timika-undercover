# ğŸ“° Indonesian News Scraper & Web Viewer

A comprehensive Indonesian news scraper that collects articles from major news sources and displays them in a beautiful web interface.

## ğŸš€ Deployment Options

### Vercel Deployment (Demo Mode)

This project is configured for Vercel deployment with sample data:

```bash
# Install Vercel CLI
npm i -g vercel

# Deploy to Vercel
vercel --prod
```

**âš ï¸ Vercel Limitations**: Read-only demo with sample data. No scheduled scraping.

### Full Functionality Deployment

For complete scraping functionality, use:
- **[Railway.app](https://railway.app)** - Recommended for Python apps
- **[Render.com](https://render.com)** - Free tier available
- **[PythonAnywhere](https://www.pythonanywhere.com)** - Python hosting

## ğŸš€ Features

- **Multi-source Scraping**: Collects news from 6 major Indonesian news sites:
  - Kompas.com
  - CNN Indonesia
  - Antara News
  - Narasi
  - Tribun News
  - Detik.com

- **Multiple Output Formats**:
  - JSON (default) - structured format with metadata
  - CSV - for spreadsheet analysis
  - Excel - for data analysis in Excel

- **Web Interface**: Beautiful, responsive web viewer with:
  - ğŸ” Search functionality
  - ğŸ“‚ Category filtering
  - ğŸ“¡ Source filtering
  - ğŸ“Š Statistics dashboard
  - ğŸ“± Mobile-responsive design

- **Advanced Features**:
  - âœ… Modular architecture - each site has its own scraper
  - âœ… Automatic duplicate removal based on URLs
  - âœ… Built-in scheduler for automated daily scraping
  - âœ… Comprehensive logging system
  - âœ… CLI interface for flexible usage
  - âœ… Random delays to respect server rate limits
  - âœ… Error handling and recovery

## ğŸ“‹ Requirements

- Python 3.7+
- Dependencies listed in `requirements.txt`

## ğŸ› ï¸ Installation

1. **Clone/Download the project**
2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Create necessary directories:**
   ```bash
   mkdir -p data logs
   ```

4. **Create configuration file (optional):**
   ```bash
   cp .env.example .env
   # Edit .env with your preferences
   ```

## ğŸ“– Usage

### 1. Run Scraping

**Scrape all sources (JSON output by default):**
```bash
python main.py
```

**Scrape specific site:**
```bash
python main.py --site detik
python main.py --site kompas
python main.py --site cnn
```

**Choose output format:**
```bash
python main.py --format json    # Default
python main.py --format csv
python main.py --format excel
```

**List available scrapers:**
```bash
python main.py --list
```

**Run with scheduler:**
```bash
python main.py --scheduler
```

### 2. Start Web Viewer

**Start the web interface:**
```bash
python web_viewer.py
```

Then open your browser and go to: `http://localhost:5000`

### 3. Environment Variables (Optional)

Create a `.env` file:

```env
# Output format: json, csv, excel
OUTPUT_FORMAT=json

# Scheduler settings
SCHEDULER_MODE=daily
SCHEDULER_TIME=08:00
SCHEDULER_INTERVAL=60
```

## ğŸ“Š Data Structure

### JSON Output Format
```json
{
  "metadata": {
    "total_articles": 150,
    "last_updated": "2025-11-04T10:30:00",
    "sources": ["Detik.com", "Kompas.com"],
    "categories": ["news", "hukum", "politik"]
  },
  "articles": [
    {
      "title": "Judul Berita",
      "url": "https://...",
      "description": "Deskripsi berita...",
      "date": "2025-11-04 10:00:00",
      "category": "news",
      "source": "Detik.com"
    }
  ]
}
```

## ğŸ“ Project Structure

```
Papua News/
â”œâ”€â”€ main.py              # Main scraper script
â”œâ”€â”€ web_viewer.py        # Flask web application
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ .env.example        # Environment variables template
â”œâ”€â”€ data/               # Output files (CSV, JSON, Excel)
â”œâ”€â”€ logs/               # Log files
â”œâ”€â”€ templates/          # HTML templates for web viewer
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ scrapers/           # Individual news site scrapers
â”‚   â”œâ”€â”€ detik_scraper.py
â”‚   â”œâ”€â”€ kompas_scraper.py
â”‚   â”œâ”€â”€ cnn_scraper.py
â”‚   â”œâ”€â”€ antara_scraper.py
â”‚   â”œâ”€â”€ narasi_scraper.py
â”‚   â””â”€â”€ tribun_scraper.py
â””â”€â”€ utils/              # Helper functions
    â”œâ”€â”€ helpers.py      # Data processing utilities
    â””â”€â”€ scheduler.py    # Task scheduling
```

## ğŸ¯ Web Interface Features

- **Search**: Search articles by title and description
- **Filtering**: Filter by news source and category
- **Statistics**: View total articles, sources, and last update time
- **Responsive**: Works on desktop and mobile devices
- **Real-time**: Auto-refresh option every 5 minutes
- **External Links**: Click to read full articles on source websites

## Data Format

Each scraped article contains the following fields:

| Field | Description |
|-------|-------------|
| title | Article title |
| date | Publication date (YYYY-MM-DD HH:MM:SS) |
| url | Article URL |
| description | Article description/excerpt |
| category | News category/topic |
| source | Source website name |

## Configuration

Create a `.env` file in the project root to customize behavior:

```env
# Output format: csv or excel
OUTPUT_FORMAT=csv

# Scheduler settings
SCHEDULER_MODE=daily
SCHEDULER_TIME=08:00
SCHEDULER_INTERVAL=60

# Optional: Custom data path
DATA_PATH=./data
```

## Scheduling

### Daily Scheduling
The scheduler can automatically run scraping at specified times:

```bash
# Schedule daily at 8:00 AM
python utils/scheduler.py --mode daily --time 08:00

# Schedule daily at 6:00 PM
python utils/scheduler.py --mode daily --time 18:00
```

### Interval Scheduling
Run scraping at regular intervals:

```bash
# Run every 30 minutes
python utils/scheduler.py --mode interval --interval 30

# Run every 2 hours
python utils/scheduler.py --mode interval --interval 120
```

## Individual Scraper Testing

Test each scraper individually:

```bash
# Test Kompas scraper
python scrapers/kompas_scraper.py

# Test CNN scraper
python scrapers/cnn_scraper.py

# Test Antara scraper
python scrapers/antara_scraper.py

# Test Narasi scraper
python scrapers/narasi_scraper.py

# Test Tribun scraper
python scrapers/tribun_scraper.py

# Test Detik scraper
python scrapers/detik_scraper.py
```

## Logging

All scraping activities are logged to `logs/scrape_log.txt` and also displayed in the console. Log levels include:
- INFO: Successful operations
- WARNING: Non-critical issues
- ERROR: Critical errors

## Output Files

- CSV files: `data/news_YYYYMMDD.csv`
- Excel files: `data/news_YYYYMMDD.xlsx`
- Site-specific files: `data/news_sitename_YYYYMMDD.csv/xlsx`

## Error Handling

The application includes robust error handling:
- Network timeouts and connection errors
- Website structure changes
- Missing or malformed data
- Rate limiting protection

## Contributing

1. Fork the repository
2. Create a new scraper in the `scrapers/` directory
3. Follow the existing pattern: implement a `scrape_<sitename>()` function
4. Add the scraper to `main.py`
5. Test thoroughly
6. Submit a pull request

## Best Practices

- âš ï¸ Respect website robots.txt files
- âš ï¸ Use appropriate delays between requests
- âš ï¸ Don't overload servers
- âš ï¸ Consider fair use policies
- âš ï¸ Monitor logs regularly

## Troubleshooting

### Common Issues

1. **No articles scraped**: Check if website structure has changed
2. **Connection timeouts**: Increase timeout values or check internet connection
3. **Rate limiting**: Increase delay values between requests
4. **Missing dependencies**: Run `pip install -r requirements.txt`

### Debug Mode

Enable debug logging by modifying `utils/helpers.py`:
```python
logging.basicConfig(level=logging.DEBUG)
```

## License

This project is for educational and research purposes. Users are responsible for complying with the terms of service of the target websites.

## Disclaimer

This tool should be used responsibly and in accordance with the terms of service of the target websites. The authors are not responsible for any misuse of this software.